{"cells":[{"cell_type":"markdown","metadata":{"id":"1fab4fc1"},"source":["# Proof of Concept Project: News Article Classification\n","---\n","## Introduction\n","The goal of this proof of concept project is to classify news articles into different categories based on their headlines. The dataset contains 422,937 news stories collected by a web aggregator between March 10th, 2014 and August 10th, 2014. The news categories include business, science and technology, entertainment, and health. Articles referring to the same news item are categorized together.\n","\n","## Project Steps\n","\n","1. **Data Gathering**\n","   - Collect the newspaper articles dataset containing headlines, URLs, and categories.\n","\n","2. **Pre-processing Functions**\n","   - Set up pre-processing functions to clean and prepare the text data for classification.\n","   - Create a stop word list to remove common words with low significance.\n","   - Handle stop-words and bigrams to improve data quality.\n","   - Apply general contractions on text to standardize language.\n","   - Tokenize and perform lemmatization, stemming as necessary to convert words to their base forms.\n","   - Remove punctuation to focus on word meanings.\n","\n","4. **Classifier Model Building**\n","   - Build the classifier model using multiple techniques found during research.\n","   - Compare the accuracy of different models to select the best approach.\n","   - Utilize basic bag of words approach for initial classification.\n","   - Implement word embedding and Word2Vec to enhance accuracy.\n","\n","5. **Model Improvement and Iteration**\n","   - Tweak contractions and pre-processing functions as needed.\n","   - Rerun the model with updated settings until accuracy reaches an optimal level.\n","\n","## Conclusion\n","This proof of concept project aims to classify news articles based on their content and categories. By following the outlined steps, we can preprocess the data, build and refine a classifier model using different techniques, and achieve accurate categorization of news articles. The incorporation of word embedding and Word2Vec will further enhance the model's performance in understanding word meanings and context.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":629,"status":"ok","timestamp":1692285224016,"user":{"displayName":"James Finbarr Kelly","userId":"03678949091753552422"},"user_tz":-60},"id":"af6b0ce9"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Fri Jun 30 10:13:51 2023\n","\n","@author: jkelly3 & sduplessis\n","\"\"\"\n","\n","#Import libraries\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","from sklearn import tree\n","from sklearn import metrics, feature_extraction, feature_selection, model_selection, naive_bayes, pipeline, manifold, preprocessing\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.metrics import accuracy_score, confusion_matrix,classification_report\n","from sklearn.svm import SVC\n","from sklearn.model_selection import cross_val_score, GridSearchCV\n","from sklearn.pipeline import Pipeline\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import SGDClassifier\n","from collections import  Counter\n","\n","import re\n","\n","#nltk.download()\n","import nltk\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer, PorterStemmer\n","from nltk.tokenize import word_tokenize\n","\n","import string\n","#import contractions as cns\n","\n","import matplotlib.pyplot as plt\n","from nltk.sentiment.vader import SentimentIntensityAnalyzer\n","\n","import gensim.models\n","from sklearn.manifold import TSNE\n","#from adjustText import adjust_text\n","\n","import xgboost as xgb\n","\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1692285228024,"user":{"displayName":"James Finbarr Kelly","userId":"03678949091753552422"},"user_tz":-60},"id":"3d52c1f3"},"outputs":[],"source":["#Download corpora\n","def download_Data(file_location):\n","\n","    '''\n","    Download and read a CSV file into a pandas DataFrame. Give the Function a string with the absolute path to your csv,\n","    or alternatively give it the name of the csv if the file is in the same directory as this script.\n","\n","    Args:\n","    file_location (str) : The file path or URL of the CSV file to be downloaded and read.\n","\n","    Returns:\n","    df (pandas.DataFrame) : A DataFrame containing the data read from the CSV file.\n","\n","    '''\n","\n","    return pd.read_csv(file_location)\n"]},{"cell_type":"code","execution_count":null,"id":"70fc58de","metadata":{},"outputs":[],"source":["# Testing on data\n","data_loc = 'C:\\Users\\conkennedy\\OneDrive\\Desktop\\textClassification\\aggregated_data.csv'\n","download_Data(data_loc)"]},{"cell_type":"markdown","metadata":{"id":"024fc5a8"},"source":["# Data Exploration:\n","---\n","Data exploration is the first step in data analysis involving the use of data visualization tools and statistical techniques to uncover data set characteristics and initial patterns.\n","\n","It is a crucially important step for both the viewer of the analysis and the data scientist. Humans are visual learners, able to process visual data much more easily than numerical data. Consequently, it's challenging for data scientists to review thousands of rows of data points and infer meaning without assistance.\n"]},{"cell_type":"markdown","metadata":{"id":"fe2280c6"},"source":["## Function: `barplot_target_category`\n","\n","This function generates a bar plot to visualize category counts within a specified DataFrame column. It supports category replacement and allows customization of plot appearance.\n","\n","## Parameters\n","\n","- **`df1`** (*pandas.DataFrame*): The original DataFrame for analysis.\n","- **`field`** (*str*): The column containing categories to visualize.\n","- **`field_replacement_dict`** (*dict*, optional): A dictionary for category replacements. Default is *None*.\n","\n","## Returns\n","\n","- **None**\n","---\n","\n","By using this function, you can conveniently visualize the distribution of categories within a specific DataFrame column. The function allows for easy handling of categories, provides options for customization, and is particularly beneficial for exploratory data analysis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"da8fbed8"},"outputs":[],"source":["#Exploratory Data Analysis\n","\n","def barplot_target_category(df1, field, field_replacement_dict=None):\n","\n","    '''\n","    Create a bar plot to visualize the counts of categories in a DataFrame column.\n","\n","    Args:\n","        df1 (pandas.DataFrame): The original DataFrame.\n","        field (str): The name of the column in the DataFrame containing the categories to be plotted.\n","        field_replacement_dict (dict, optional): A dictionary to map category replacements if needed. Default is None.\n","\n","    Returns:\n","        None\n","    '''\n","\n","    df = df1.copy()\n","\n","    if field_replacement_dict:\n","        df[field] = df[field].replace(field_replacement_dict)\n","\n","    counts = df[field].value_counts()\n","    num_categories = len(counts)\n","    color_palette = mpl.colormaps.get_cmap(\"tab10\")  # Change 'tab10' to any other colormap you prefer\n","\n","    # Create the bar plot\n","    plt.bar(counts.index, counts.values, color=color_palette(range(num_categories)))\n","    plt.plot()\n","\n","    # Customize the plot\n","    plt.xlabel('Category')\n","    plt.ylabel('Count')\n","    plt.title('Counts of Categories')\n","    plt.xticks(rotation=45)\n","\n","    # Display the plot\n","    plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"27618fc6"},"source":["## Analysis of N-grams\n","\n","## Function: `_get_top_ngram`\n","\n","This function retrieves the top n-grams (word sequences) from a corpus based on their frequency. This function can be called\n","on the unprocesesed target column or after processing.\n","\n","### Parameters\n","\n","- **`corpus`** (*list*): A list of strings representing the text corpus.\n","- **`n`** (*int*, optional): The n-gram size. Default is None, corresponding to unigrams (single words).\n","\n","### Returns\n","\n","- **list**: A list of tuples containing the top n-grams and their frequencies.\n","\n","---\n","\n","## Function: `plot_top_ngrams_barchart`\n","\n","This function creates a bar chart to visualize the top n-grams (word sequences) in the given text.\n","\n","### Parameters\n","\n","- **`text`** (*pandas.Series*): A pandas Series containing the text data.\n","- **`n`** (*int*, optional): The n-gram size. Default is 2.\n","\n","\n","---\n","\n","By utilizing these functions, you can efficiently analyze the distribution of n-grams within text data. The `_get_top_ngram` function calculates the most frequent n-grams, while `plot_top_ngrams_barchart` visualizes these n-grams using a bar chart."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4538034d"},"outputs":[],"source":["#Analysis of N-grams:\n","\n","def _get_top_ngram(corpus, n=None):\n","\n","    '''\n","    Get the top n-grams (word sequences) in a corpus based on their frequency.\n","\n","    Args:\n","        corpus (list): A list of strings representing the text corpus.\n","        n (int, optional): The n-gram size. Default is None, which corresponds to unigrams (single words).\n","\n","    Returns:\n","        list: A list of tuples containing the top n-grams and their frequencies.\n","    '''\n","\n","    # Create a CountVectorizer object to convert text into a bag of words representation\n","    vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n","\n","    # Transform the corpus into a bag of words\n","    bag_of_words = vec.transform(corpus)\n","\n","    # Sum the word frequencies across the entire corpus\n","    sum_words = bag_of_words.sum(axis=0)\n","\n","    # Create a list of tuples containing each n-gram and its frequency\n","    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n","\n","    # Sort the list in descending order based on the n-gram frequency\n","    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n","\n","    # Return the top n-grams (defaulting to the top 10)\n","    return words_freq[:10]\n","\n","# Plot the N-grams:\n","\n","def plot_top_ngrams_barchart(text, n=2):\n","\n","    '''\n","    Create a bar chart to visualize the top n-grams (word sequences) in the given text.\n","\n","    Args:\n","        text (pandas.Series): A pandas Series containing the text data.\n","        n (int, optional): The n-gram size. Default is 2.\n","\n","    Returns:\n","        None\n","    '''\n","\n","    # Get the set of English stopwords\n","    stop = set(stopwords.words('english'))\n","\n","    # Split the text into words and remove stopwords\n","    new = text.str.split()\n","    new = new.values.tolist()\n","    corpus = [word for i in new for word in i if word not in stop]\n","\n","    # Get the top n-grams using the _get_top_ngram function\n","    top_n_bigrams = _get_top_ngram(text, n)[:10]\n","\n","    # Extract the n-grams and their frequencies from the result\n","    x, y = map(list, zip(*top_n_bigrams))\n","\n","    # Create a bar plot using seaborn's barplot function\n","    sns.barplot(x=y, y=x)\n","\n","    # Show the plot\n","    plt.show()\n","\n","#plot_top_ngrams_barchart(df['TITLE'],2)\n","\n","#plot_top_ngrams_barchart(df['CLEAN_TEXT'],3)\n"]},{"cell_type":"markdown","metadata":{"id":"128c7dcb"},"source":["# Functions Used in Data Manipulation and Preprocessing\n","---\n","These functions collectively support the preparation and preprocessing of text data for classification tasks, incorporating steps such as forming corpora, handling contractions, removing stopwords, cleaning text, and engineering features for improved model accuracy.Use help() on the functions or read the docstrings for more information."]},{"cell_type":"markdown","metadata":{"id":"0331919d"},"source":["## Form a Corpus List from a target Column:\n","\n","1. **`form_corpora_list(df, field, is_list=False)`**\n","   \n","    - **Description:** Extracts a list of words from a DataFrame column for text analysis.\n","   \n","    - **Parameters:**\n","        - `df` (pandas DataFrame): The DataFrame containing the data.\n","        - `field` (str): The name of the column from which the corpus will be extracted.\n","        - `is_list` (bool): Flag indicating if the column contains lists of words or string type variables.\n","   \n","    - **Returns:** A list containing the words from the specified column (corpus)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9d37b978"},"outputs":[],"source":["# Form the Corpora and Analyse frequency of non-stop words:\n","def form_corpora_list(df, field, is_list = False):\n","\n","    '''\n","    Extract a corpus list from a DataFrame column. Give this function the pandas dataframe and the field/column that\n","    you are using to form the corpus of your analysis. If that column is tokenized e.g., in a list format, include the\n","    is_list flag as true, if it is a column of strings, make the flag false.\n","\n","    Args:\n","    df (pandas.DataFrame) : The DataFrame containing the data.\n","\n","    field (str) : The name of the column in the DataFrame from which the corpus will be extracted.\n","\n","    is_list (bool), optional (default=False) :\n","        If True, assumes that the specified column contains lists of words.\n","        If False, assumes that the specified column contains sentences or text.\n","\n","    Returns:\n","    list (list) : A list containing the words or elements from the specified column (corpus).\n","    '''\n","\n","    if is_list:\n","        return df[field].explode().tolist()\n","\n","    corpus = []\n","    column_split = df[field].str.split()\n","    column_split = column_split.values\n","    column_split_list = column_split.tolist()\n","\n","    # Flatten nested Lists into one corpus list\n","    for i in column_split:\n","        for word in i:\n","            corpus.append(word)\n","\n","\n","    return corpus\n"]},{"cell_type":"markdown","metadata":{"id":"0a61dc62"},"source":["## Function to define the specific contractions in the dataset:\n","2. **`specific_contractions()`**\n","   \n","    - **Description:** Provides a dictionary of specific contractions and their replacements.\n","   \n","    - **Returns:** A dictionary of contraction patterns and their corresponding replacements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d1e1f46"},"outputs":[],"source":["def specific_contractions():\n","\n","    '''\n","    Get a dictionary of specific contractions and their corresponding replacements.\n","\n","    Returns:\n","    general phrases (dict) : A dictionary containing regular expression patterns as keys and their corresponding replacements\n","    as values.\n","\n","    '''\n","\n","    GENERAL_PHRASES = {\n","        #covid\n","        r\"\\bcovid19\\b\": \"covid\",\n","        r\"\\bcovid 19\\b\": \"covid\",\n","        r\"\\bcoronavirus\\b\": \"covid\",\n","        r\"\\bcorona virus\\b\": \"covid\",\n","        r\"\\b[Uu]\\.?[Ss]\\.?'?s?\\b \": \"US \",\n","        r\"\\bU\\.?K\\.?\\b\" : \"United Kingdom\",\n","        r\"&\" : \"and\"}\n","\n","    return GENERAL_PHRASES\n"]},{"cell_type":"markdown","metadata":{"id":"d4561a16"},"source":["## Replace the Custom Contractions defined in previous Function:\n","3. **`replace_custom_contractions(text)`**\n","   \n","    - **Description:** Replaces specific contractions in the given text with their corresponding expansions.\n","   \n","    - **Parameters:** `text` (str): The input text with contractions to be replaced.\n","   \n","    - **Returns:** The input text with specific contractions replaced."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c06d867f"},"outputs":[],"source":["def replace_custom_constractions(text):\n","\n","    '''\n","    Replace specific contractions in the given text with their corresponding expansions.\n","\n","    Args:\n","    text (str) : The input text where contractions will be replaced.\n","\n","    Returns:\n","    text (str) : The input text with specific contractions replaced.\n","\n","    '''\n","\n","    dict_contractions = specific_contractions()\n","\n","    for pattern, replacement in dict_contractions.items():\n","        text = re.sub(pattern, replacement, text)\n","\n","    return text\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5685f8c4"},"source":["## Form a set of stopwords with the built-in nltk stopwords() and any aditional stopward identified:\n","4. **`custom_stopwords()`**\n","   \n","    - **Description:** Provides a set of custom stopwords by combining NLTK's English stopwords with additional stopwords.\n","   \n","    - **Returns:** A set of custom stopwords for text processing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cb8e081b"},"outputs":[],"source":["#Import stopwords\n","def custom_stopwords():\n","\n","    '''\n","    Get a set of custom stopwords by combining NLTK's English stopwords with additional stopwords.\n","\n","    Returns:\n","        set (set): A set containing custom stopwords for text processing.\n","    '''\n","\n","    from nltk.corpus import stopwords\n","    stopwords = set(stopwords.words('english'))\n","\n","\n","    extra = set([])# Placeholder to include regular expressions and custom stopwords later\n","\n","    stopwords = stopwords.union(extra)\n","    return stopwords\n"]},{"cell_type":"markdown","metadata":{"id":"10df792e"},"source":["## Sentiment Analysis with TextBlob\n","\n","The provided functions enable sentiment analysis using the TextBlob library, offering a convenient way to assess text sentiment and subjectivity.\n","\n","### Function: `get_polarity`\n","\n","This function calculates the polarity (sentiment) of a given text using TextBlob.\n","\n","- **Parameters**:  \n","  - `text` (*str*): The input text.\n","\n","- **Returns**:  \n","  - *float*: The polarity score ranging from -1.0 (negative) to 1.0 (positive). If sentiment analysis fails, it returns 0.0.\n","\n","### Function: `get_subjectivity`\n","\n","This function calculates the subjectivity of a given text using TextBlob.\n","\n","- **Parameters**:  \n","  - `text` (*str*): The input text.\n","\n","- **Returns**:  \n","  - *float*: The subjectivity score ranging from 0.0 (objective) to 1.0 (subjective). If sentiment analysis fails, it returns 0.0.\n","\n","These functions offer a streamlined approach for analyzing sentiment and subjectivity of text using TextBlob. They can be valuable for tasks like understanding customer feedback sentiment, monitoring social media sentiment, and assessing the subjective nature of textual content. They are called in the feature engineering function.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"577d5038"},"outputs":[],"source":["from textblob import TextBlob\n","\n","def get_polarity(text):\n","    \"\"\"\n","    Calculate the polarity (sentiment) of a given text using TextBlob.\n","\n","    Args:\n","        text (str): The input text.\n","\n","    Returns:\n","        float: The polarity score ranging from -1.0 (negative) to 1.0 (positive).\n","               If sentiment analysis fails, returns 0.0.\n","    \"\"\"\n","    try:\n","        textblob = TextBlob(text)\n","        polarity = textblob.sentiment.polarity\n","    except:\n","        polarity = 0.0\n","    return polarity\n","\n","def get_subjectivity(text):\n","    \"\"\"\n","    Calculate the subjectivity of a given text using TextBlob.\n","\n","    Args:\n","        text (str): The input text.\n","\n","    Returns:\n","        float: The subjectivity score ranging from 0.0 (objective) to 1.0 (subjective).\n","               If sentiment analysis fails, returns 0.0.\n","    \"\"\"\n","    try:\n","        textblob = TextBlob(text)\n","        subjectivity = textblob.sentiment.subjectivity\n","    except:\n","        subjectivity = 0.0\n","    return subjectivity\n"]},{"cell_type":"markdown","metadata":{"id":"b05cc6fb"},"source":["## Preprocess Function:\n","Please note that the built in `preprocessing()` function from skilearn will do similar processing but I wanted to provide a custom function that can be tailored to any NLP project as a template.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6d58f186"},"outputs":[],"source":["def pre_process(text, is_lemming = False, is_stemming = False, str_rejoin = False):\n","\n","    '''\n","    Preprocess the text, giving options for lemmatization, stemming and rejoining string from list as optional flags.\n","\n","    Args:\n","        text (str): The string input text that is being preprocessed.\n","        is_lemming (bool): The flag to control if the text should be lemmatized - default is False.\n","        is_stemming (bool): The flag to control if the text should be stemmed - default is False.\n","        str_rejoin (bool): The flag to control if the text should be rejoined from a list at the end of the preprocessing - default is False.\n","\n","    Returns:\n","        str: A string with the preprocessed text.\n","\n","    '''\n","\n","\n","    # Remove the basic contractions with the contractions package:\n","    text =  replace_custom_constractions(text)\n","    text = cns.fix(text)\n","\n","    # Remove 1st, 2nd, 3rd etc.:\n","\n","    text = re.sub(r\"\\b\\d+(st|nd|rd|th)\\b\", '', text)\n","\n","    # Remove the Amounts of money e.g. 100k, 200M etc.:\n","    text = re.sub(r\"\\b\\d+(\\.\\d+)?[kMBGTPEZY]\\b\", '', text)\n","\n","\n","    # Remove \" 's \" pattern:\n","    text = re.sub(r\"\\b\\w+'s\\b\", '', text)\n","\n","    # remove currency numbers:\n","    text = re.sub(r\"[$€¥£]\\d+[^ ]*\", '', text)\n","\n","\n","    # Remove punctuation where the punctuation is internal - add space instead to maintain word breakpoint:\n","    pattern = r\"(?<=\\w)[!\\\"#$%&'()*+,-./:;<=>?@[\\\\\\]^_`{|}~](?=\\w)\"\n","    text = re.sub(pattern, ' ', text)\n","\n","    # Remove punctuation internal to words and replace with space\n","    text = re.sub(r\"[^a-zA-Z0-9\\s]+\", '', text)\n","\n","    # Remove Numbers:\n","    text =  re.sub(r'\\d+', '', text)\n","\n","\n","    # Remove multiple spaces where they occur:\n","    text = re.sub(r'^\\s+', '', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    # Make the Text Lower\n","    text = text.lower()\n","\n","    # Tokenize the text:\n","    text = nltk.word_tokenize(text)\n","\n","    # Remove set of default stopwords with stopwords library:\n","    stop = custom_stopwords()\n","    text = [word for word in text if word not in stop]\n","\n","    # Lemmatisation:\n","    if is_lemming:\n","\n","        lem = WordNetLemmatizer()\n","        text = [lem.lemmatize(w) for w in text]\n","\n","    # Stemming\n","    if is_stemming:\n","\n","        ps = PorterStemmer()\n","        text = [ps.stem(w) for w in text]\n","\n","    # If you want the tokens to be made back into a string:\n","    if str_rejoin:\n","        text = \" \".join(text)\n","\n","\n","    return text\n"]},{"cell_type":"markdown","metadata":{"id":"691b973f"},"source":["## Preprocessing Function applied to a Dataframe:\n","5. **`data_frame_pre_process(df, field, is_lemming=False, is_stemming=False, str_rejoin=False)`**\n","   \n","    - **Description:** Preprocesses text data in a DataFrame column.\n","   \n","    - **Parameters:**\n","        - `df_input` (pandas DataFrame): The DataFrame containing the text data.\n","        - `field` (str): The column containing the text data.\n","        - `is_lemming` (bool): Flag for lemmatization.\n","        - `is_stemming` (bool): Flag for stemming.\n","        - `str_rejoin` (bool): Flag to rejoin tokens into a string.\n","   \n","    - **Returns:** A DataFrame with preprocessed text data in a new 'CLEAN_TEXT' column."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fe9f9375"},"outputs":[],"source":["def data_frame_pre_process(df_input, field, is_lemming = False, is_stemming = False, str_rejoin = False):\n","\n","    '''\n","    Preprocess the text data in a DataFrame column.\n","\n","    Args:\n","        df_input (pandas.DataFrame): The DataFrame containing the text data.\n","        field (str): The name of the column in the DataFrame containing the text data.\n","        is_lemming (bool): The flag to control if the text should be lemmatized - default is False.\n","        is_stemming (bool): The flag to control if the text should be stemmed - default is False.\n","        str_rejoin (bool): The flag to control if the text should be rejoined from a list at the end of the preprocessing - default is False.\n","\n","    Returns:\n","        pandas.DataFrame: A DataFrame with the preprocessed text data in a new column named 'CLEAN_TEXT'.\n","    '''\n","\n","    # Remove NA values from the dataset if present:\n","    df = df_input.dropna().copy()\n","\n","    # Remove the basic contractions with the contractions package:\n","    df['CLEAN_TEXT'] = df[field].apply(lambda x: replace_custom_constractions(x))\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x: cns.fix(x))\n","\n","    # Remove 1st, 2nd, 3rd etc.:\n","\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r\"\\b\\d+(st|nd|rd|th)\\b\", '', x))\n","\n","    # Remove the Amounts of money e.g. 100k, 200M etc.:\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r\"\\b\\d+(\\.\\d+)?[kMBGTPEZY]\\b\", '', x))\n","\n","\n","    # Remove \" 's \" pattern:\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r\"\\b\\w+'s\\b\", '', x))\n","\n","    # remove currency numbers:\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r\"[$€¥£]\\d+[^ ]*\", '', x))\n","\n","\n","    # Remove punctuation where the punctuation is internal - add space instead to maintain word breakpoint:\n","    pattern = r\"(?<=\\w)[!\\\"#$%&'()*+,-./:;<=>?@[\\\\\\]^_`{|}~](?=\\w)\"\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x: re.sub(pattern, ' ', x))\n","\n","    # Remove punctuation internal to words and replace with space\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r\"[^a-zA-Z0-9\\s]+\", '', x))\n","\n","    # Remove Numbers:\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r'\\d+', '', x))\n","\n","\n","    # Remove multiple spaces where they occur:\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r'^\\s+', '', x))\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : re.sub(r'\\s+', ' ', x))\n","\n","    # Make the Text Lower\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : x.lower())\n","\n","    # Tokenize the text:\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : nltk.word_tokenize(x))\n","\n","    # Remove set of default stopwords with stopwords library:\n","    stop = custom_stopwords()\n","    df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : [word for word in x if word not in stop])\n","\n","    # Lemmatisation:\n","    if is_lemming:\n","\n","        lem = WordNetLemmatizer()\n","        df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : [lem.lemmatize(w) for w in x])\n","\n","    # Stemming\n","    if is_stemming:\n","\n","        ps = PorterStemmer()\n","        df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : [ps.stem(w) for w in x])\n","\n","    # If you want the tokens to be made back into a string:\n","    if str_rejoin:\n","        df['CLEAN_TEXT'] = df['CLEAN_TEXT'].apply(lambda x : \" \".join(x))\n","\n","    return df\n"]},{"cell_type":"markdown","metadata":{"id":"92101ba0"},"source":["## Feature Engineering:\n","---\n","Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. In order to make machine learning work well on new tasks, it might be necessary to design and train better features.\n","\n","In this case, the feature engineering is impractical and does not lead to further accuracy, but including an example is always useful for applying to non-text classification problems or problems with a more meaningful relationship with the features I engineer below e.g. classifying complaint might benefit from knowing punctuation or length of sentence etc.\n","\n","6. **`feature_engineer(df, target, unwanted=False)`**\n","   \n","    - **Description:** Performs feature engineering on the DataFrame for improved classification accuracy.\n","   \n","    - **Parameters:**\n","        - `df` (pandas DataFrame): The DataFrame containing pre-processed text data.\n","        - `target` (str): Name of the target categorical label.\n","        - `unwanted` (list): List of unwanted columns to drop.\n","   \n","    - **Returns:**\n","        - `X` (pandas DataFrame): DataFrame with added features.\n","        - `y` (numpy array): Label encoded target.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"077abc67"},"outputs":[],"source":["def feature_engineer(df,target, unwanted = False):\n","\n","    \"\"\"\n","    Perform feature engineering on the given DataFrame for increased accuracy in classification.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame containing pre-processed text data.\n","        target (str): The String name of the target categorical Label.\n","        unwanted (list): The list of unwanted columns Strings to drop\n","\n","    Returns:\n","        X (pandas.DataFrame): The DataFrame with added features based on feature engineering.\n","        Y (np.array): Label Encoding of Categorical Target\n","\n","    Note:\n","        This function adds various features to the DataFrame including word count, character count,\n","        diversity score, punctuation count, polarity, subjectivity, counts of specific parts of speech (POS) tags,\n","        and more.\n","\n","    \"\"\"\n","    y = df[target]\n","\n","\n","    # Feature Engineering for increased accuracy in classification:\n","    # Now we have processed and pre-processed text in our dataframe. Let's start making features from the above data.\n","    if unwanted:\n","        unwanted.append(target)\n","        X = df.drop(unwanted, axis = 1)\n","\n","    # Feature 1 - Length of the input OR count of the words in the statement (Vocab size).\n","    X['WORD_COUNT'] = X['TITLE'].apply(lambda x: len(str(x).split()))  # Feature 1\n","\n","    # Feature 2 - Count of characters in a statement\n","    X['CHARACTER_COUNT'] = X['TITLE'].apply(lambda x: len(str(x)))  # Feature 2\n","\n","    # Feature 3 - Diversity_score i.e. Average length of words used in statement\n","    X['AVERAGE_LENGTH'] = X['CHARACTER_COUNT'] / X['WORD_COUNT']  # Feature 3\n","\n","    # Feature 4: Count of punctuations in the input.\n","    X['PUNCTUATION_COUNT'] = X['TITLE'].apply(lambda x: len([w for w in str(x) if w in string.punctuation]))  # Feature 4\n","\n","    # Change df_small to df to create these features on the complete dataframe\n","    X['polarity'] = X['TITLE'].apply(get_polarity)  # Feature 5: Polarity\n","    X['subjectivity'] = X['TITLE'].apply(get_subjectivity)  # Feature 6: Subjectivity\n","\n","    # Tokenize all text without stopwords\n","    all_text_without_sw = ''\n","    for i in df.itertuples():\n","        all_text_without_sw = all_text_without_sw + str(i.TITLE)\n","\n","    tokenized_all_text = word_tokenize(all_text_without_sw)  # tokenize the text\n","\n","    # Adding POS Tags to tokenized words\n","    list_of_tagged_words = nltk.pos_tag(tokenized_all_text)\n","    set_pos = set(list_of_tagged_words)  # set of POS tags & words\n","\n","    # Counting specific POS tags\n","    nouns = ['NN', 'NNS', 'NNP', 'NNPS']  # POS tags of nouns\n","    list_of_words = set(map(lambda tuple_2: tuple_2[0], filter(lambda tuple_2: tuple_2[1] in nouns, set_pos)))\n","    X['NOUN'] = X['TITLE'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]))  # Feature 7\n","\n","    # Counting pronouns\n","    pronouns = ['PRP', 'PRP$', 'WP', 'WP$']  # POS tags of pronouns\n","    list_of_words = set(map(lambda tuple_2: tuple_2[0], filter(lambda tuple_2: tuple_2[1] in pronouns, set_pos)))\n","    df['PRONOUN_COUNT'] = df['TITLE'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]))  # Feature 8\n","\n","    # Counting verbs\n","    verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # POS tags of verbs\n","    list_of_words = set(map(lambda tuple_2: tuple_2[0], filter(lambda tuple_2: tuple_2[1] in verbs, set_pos)))\n","    X['VERBS_COUNT'] = X['TITLE'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]))  # Feature 9\n","\n","    # Counting adverbs\n","    adverbs = ['RB', 'RBR', 'RBS', 'WRB']  # POS tags of adverbs\n","    list_of_words = set(map(lambda tuple_2: tuple_2[0], filter(lambda tuple_2: tuple_2[1] in adverbs, set_pos)))\n","    X['ADVERBS_COUNT'] = X['TITLE'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]))  # Feature 10\n","\n","    # Counting adjectives\n","    adjectives = ['JJ', 'JJR', 'JJS']  # POS tags of adjectives\n","    list_of_words = set(map(lambda tuple_2: tuple_2[0], filter(lambda tuple_2: tuple_2[1] in adjectives, set_pos)))\n","    X['ADJECTIVE_COUNT'] = X['TITLE'].apply(lambda x: len([w for w in str(x).lower().split() if w in list_of_words]))  # Feature 11\n","\n","    encoder = LabelEncoder()\n","    y = encoder.fit_transform(y)\n","\n","    return X,y"]},{"cell_type":"markdown","metadata":{"id":"5b11a60b"},"source":["# Further Data Exploration on Prepared Dataset:\n","---\n","Now that we have our target text processed, we should take another exploratory look at the data in its final form before modelling:"]},{"cell_type":"markdown","metadata":{"id":"8028cfb2"},"source":["## Word Frequency Analysis without stopwords\n","\n","This function conducts a word frequency analysis on a corpus within a DataFrame column and provides visualization of the results.\n","\n","## Function: `word_frequency_analysis`\n","\n","### Parameters\n","\n","- **`df`** (*pandas.DataFrame*): The DataFrame containing the text data.\n","- **`field`** (*str*): The column name in the DataFrame with the text data.\n","- **`amount`** (*int* or `'all'`): The number of most common words to analyze. Use `'all'` to analyze all words.\n","- **`is_list`** (*bool*, optional): Set to *True* if the specified column contains lists of words. Default is *False*.\n","- **`dict_wanted`** (*bool*, optional): Set to *True* to return a dictionary of word frequencies. Default is *False*.\n","\n","### Returns\n","\n","- **None** or **dict**: If *dict_wanted* is *True*, returns a dictionary of word frequencies. Otherwise, displays a bar plot.\n","\n","### Procedure\n","\n","1. **Form Corpora List**: Obtain a list of text corpora using the *form_corpora_list* function, considering the *is_list* parameter.\n","\n","2. **Count Most Common Words**: Utilize the *Counter* class to count the most common words within the corpus.\n","\n","3. **Initialize Lists**: Prepare lists to hold word and frequency information.\n","\n","4. **Determine Range**: Based on the *amount* parameter, decide the range for analysis (up to specified count or all words).\n","\n","5. **Extract Information**: Extract word and frequency data from the most common list based on the determined range.\n","\n","6. **Check for Dictionary**: If *dict_wanted* is *True*, return a dictionary of word frequencies.\n","\n","7. **Create Bar Plot**: If *dict_wanted* is *False*, generate a bar plot using seaborn's *barplot* function and display it.\n","\n","---\n","\n","By utilizing this function, you can conduct word frequency analysis on textual data in a DataFrame column. The function allows customization of the number of most common words to analyze and provides the option to return a dictionary of word frequencies. The resulting bar plot aids in visualizing the frequency distribution of words.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"740bfa56"},"outputs":[],"source":["# Word Frequency Analysis without stopwords:\n","\n","def word_frequency_analysis(df, field, amount, is_list = False, dict_wanted = False):\n","\n","    '''\n","    Perform word frequency analysis on a corpus in a DataFrame column and visualize the results.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame containing the text data.\n","        field (str): The name of the column in the DataFrame containing the text data.\n","        amount (int or 'all'): The number of most common words to analyze. Set to 'all' for all words.\n","        is_list (bool, optional): Set to True if the specified column contains lists of words. Default is False.\n","        dict_wanted (bool, optional): Set to True to return a dictionary of word frequencies. Default is False.\n","\n","    Returns:\n","        None or dict: If dict_wanted is True, returns a dictionary of word frequencies. Otherwise, displays a bar plot.\n","    '''\n","\n","    corpus = form_corpora_list(df, field, is_list)\n","\n","    # Count most common words in the corpus\n","    counter = Counter(corpus)\n","    most_common_list = counter.most_common()\n","\n","    # Initialize lists to store word and frequency information\n","    list_a, list_b = [], []\n","\n","    # Determine the range for analysis based on 'amount' parameter\n","    if amount == 'all':\n","        range_limit = len(most_common_list)\n","    else:\n","        range_limit = amount\n","\n","    # Extract word and frequency information based on range_limit\n","    for word, count in most_common_list[:range_limit]:\n","        list_a.append(word)\n","        list_b.append(count)\n","\n","    # Check if a dictionary of word frequencies is wanted\n","    if dict_wanted:\n","        return {list_a[i]: list_b[i] for i in range(len(list_a))}\n","\n","    # Create a bar plot using seaborn's barplot function\n","    sns.barplot(x=list_b, y=list_a)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"850e3fba"},"source":["## Word Frequency Analysis with Specific Word Frequency Request\n","\n","This function performs word frequency analysis on a corpus within a DataFrame column and displays the frequency of specific words using a dictionary.\n","\n","## Function: `Word_frequency_request`\n","\n","### Parameters\n","\n","- **`df`** (*pandas.DataFrame*): The DataFrame containing the text data.\n","- **`field`** (*str*): The column name in the DataFrame with the text data.\n","- **`test_list`** (*list*, optional): A list of words to check the frequency for. Default is an empty list.\n","- **`is_list`** (*bool*, optional): Set to *True* if the specified column contains lists of words. Default is *False* (for strings).\n","\n","### Returns\n","\n","- **None**\n","\n","### Procedure\n","\n","1. **Word Frequency Analysis**: Invoke the *word_frequency_analysis* function to retrieve a dictionary of word frequencies from the corpus.\n","\n","2. **Iterate Over Test Words**: Iterate through each word in the *test_list*.\n","\n","3. **Check for Presence**: For each word, check if it exists in the word frequency dictionary.\n","\n","4. **Print Frequency**: If the word is present, print its frequency. If not, indicate that the word is not present in the corpus.\n","\n","---\n","\n","By using this function, you can analyze the word frequency within a DataFrame column and retrieve the frequency of specific words. The function utilizes the previously defined *word_frequency_analysis* function and provides a convenient way to check the occurrence of specific words in the corpus.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37117e7a"},"outputs":[],"source":["def Word_frequency_request(df, field, test_list = [], is_list = False):\n","\n","    '''\n","    Perform word frequency analysis on a corpus in a DataFrame column and display the frequency of specific words\n","    using a dictionary.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame containing the text data.\n","        field (str): The name of the column in the DataFrame containing the text data.\n","        test_list (list, optional): A list of words to check the frequency for. Default is an empty list.\n","        is_list (bool, optional): Set to True if the specified column contains lists of words. Default is False (for strings).\n","\n","    Returns:\n","        None\n","    '''\n","\n","    word_dict = word_frequency_analysis(df, field, all, is_list, True)\n","\n","    for word in test_list:\n","\n","        if word in word_dict.keys():\n","            print(f\"Frequency of {word}: {word_dict[word]}\")\n","\n","        else:\n","            print(f\"{word} is not present in this corpus\")\n"]},{"cell_type":"markdown","metadata":{"id":"edf08dcc"},"source":["## Sentiment Analysis with VADER\n","\n","This function performs sentiment analysis on a given text using the VADER (Valence Aware Dictionary and Sentiment Reasoner) sentiment analysis tool.\n","\n","## Function: `sentiment_vader`\n","\n","### Parameters\n","\n","- **`text`** (*str*): The input text for sentiment analysis.\n","- **`sid`** (*nltk.sentiment.vader.SentimentIntensityAnalyzer*): The VADER SentimentIntensityAnalyzer instance.\n","\n","### Returns\n","\n","- A sentiment label with the highest score from the VADER analysis ('pos', 'neg', or 'neu').\n","\n","### Procedure\n","\n","1. **Polarity Score Calculation**: Calculate the polarity scores using the VADER SentimentIntensityAnalyzer.\n","\n","2. **Select the Maximum Score**: Identify the sentiment label with the highest score.\n","\n","### Function: `plot_sentiment_barchart`\n","\n","This function plots a bar chart to visualize the distribution of sentiments in the given text data.\n","\n","### Parameters\n","\n","- **`text`** (*pandas.Series*): A pandas Series containing the text data.\n","\n","### Returns\n","\n","- **None**\n","\n","### Procedure\n","\n","1. **Sentiment Analysis**: Initialize the VADER SentimentIntensityAnalyzer and analyze sentiment for each text using the `sentiment_vader` function.\n","\n","2. **Plot Distribution**: Create a bar plot to visualize the sentiment distribution.\n","\n","By using these functions, you can perform sentiment analysis on text data and visualize the distribution of sentiments. The `sentiment_vader` function computes sentiment labels, and the `plot_sentiment_barchart` function offers an easy way to visualize the sentiment distribution in the provided text data. Remember to have the `nltk` library installed and download the VADER lexicon using `nltk.download('vader_lexicon')` if needed.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7c2a6c77"},"outputs":[],"source":["#Sentiment analysis\n","\n","def sentiment_vader(text, sid):\n","\n","    \"\"\"\n","    Analyze the sentiment of a given text using the VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiment analysis tool.\n","\n","    Args:\n","        text (str): The input text for sentiment analysis.\n","        sid (nltk.sentiment.vader.SentimentIntensityAnalyzer): The VADER SentimentIntensityAnalyzer instance.\n","\n","    Returns:\n","        str: The sentiment label with the highest score from the VADER analysis (either 'pos', 'neg', or 'neu').\n","    \"\"\"\n","\n","    # Polarity score returns dictionary\n","    ss = sid.polarity_scores(text)\n","    ss.pop('compound')\n","    return max(ss, key=ss.get)\n","\n","\n","def plot_sentiment_barchart(text):\n","\n","    \"\"\"\n","    Plot a bar chart to visualize the distribution of sentiments in the given text data.\n","\n","    Args:\n","        text (pandas.Series): A pandas Series containing the text data.\n","\n","    Returns:\n","        None\n","    \"\"\"\n","    # Initialize the VADER SentimentIntensityAnalyzer - #nltk.download('vader_lexicon') if necessary:\n","    sid = SentimentIntensityAnalyzer()\n","\n","    # Analyze sentiment for each text using the sentiment_vader function\n","    sentiment = text.map(lambda x: sentiment_vader(x, sid=sid))\n","\n","    # Create a bar plot to visualize the sentiment distribution\n","    plt.bar(sentiment.value_counts().index, sentiment.value_counts())\n","\n","\n","#plot_sentiment_barchart(df_test['CLEAN_TEXT'])"]},{"cell_type":"markdown","metadata":{"id":"bdb03d09"},"source":["# Data Preparation: Forming Feature Matrix and Target Vector\n","---\n","Now that we have preparedt the data and looked at some of the patterns and behaviour of the data, it's time to from our feature matrix and our target vector. This function prepares the feature matrix (*X*) and target vector (*y*) for a given DataFrame and target column. It also supports unwanted column removal and optional feature engineering.\n","\n","## Function: `Form_X_y`\n","\n","### Parameters\n","\n","- **`df`** (*pandas.DataFrame*): The DataFrame containing the data.\n","- **`target`** (*str*): The target column name.\n","- **`unwanted`** (*bool*, optional): Set to *True* if unwanted columns need to be removed from the feature matrix. Default is *False*.\n","- **`feat_eng_flag`** (*bool*, optional): Set to *True* to enable feature engineering. Default is *False*.\n","\n","### Returns\n","\n","- **X** (*pandas.DataFrame*): The feature matrix.\n","- **y** (*numpy.array*): The target vector.\n","\n","### Procedure\n","\n","1. **Data Preprocessing**: If specified, perform data preprocessing on the DataFrame using the `download_Data` and `data_frame_pre_process` functions.\n","\n","2. **Feature Engineering (Optional)**: If `feat_eng_flag` is *True*, invoke the `feature_engineer` function to obtain feature-engineered *X* and *y*. Measure and display the time taken.\n","\n","3. **Obtain Target Vector**: Extract the target column (*target*) as the target vector (*y*).\n","\n","4. **Remove Unwanted Columns (Optional)**: If *unwanted* is *True*, remove unwanted columns and assign the remaining DataFrame to *X*.\n","\n","5. **Label Encoding**: Encode the target vector (*y*) using *LabelEncoder*.\n","\n","By using this function, you can easily prepare the feature matrix and target vector for machine learning tasks. You can also opt for optional unwanted column removal and feature engineering if required.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cbb7197"},"outputs":[],"source":["def Form_X_y(df, target, unwanted = False, feat_eng_flag = False):\n","\n","    \"\"\"\n","    Prepare the feature matrix (X) and target vector (y) for a given DataFrame and target column.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame containing the data.\n","        target (str): The target column name.\n","        unwanted (bool, optional): Set to True if unwanted columns need to be removed from the feature matrix. Default is False.\n","        feat_eng_flag (bool, optional): Set to True to enable feature engineering. Default is False.\n","\n","    Returns:\n","        X (pandas.DataFrame): The feature matrix.\n","        y (numpy.array): The target vector.\n","    \"\"\"\n","\n","    # If feat_eng_flag is True, perform feature engineering and return X and y:\n","    if feat_eng_flag:\n","\n","        start = time.time()\n","        X, y = feature_engineer(df, 'CATEGORY', unwanted)\n","        print('Time took Feature Engineer: ', time.time() - start, 'seconds')\n","        return X,y\n","\n","    # Extract the target vector y from the specified column:\n","    y = df[target]\n","    X = df.drop(target, axis = 1).copy()\n","\n","    # If unwanted is True, remove specified columns from the DataFrame:\n","    if unwanted:\n","        X = X.drop(unwanted, axis = 1)\n","\n","    # Encode the target vector y using LabelEncoder:\n","    encoder = LabelEncoder()\n","    #y = encoder.fit_transform(y)\n","    return X, y\n","\n","\n","#X,y = Form_X_y(df, 'CATEGORY', ['URL','PUBLISHER','STORY','HOSTNAME','TIMESTAMP'], False)"]},{"cell_type":"markdown","metadata":{"id":"614cd667"},"source":["# Defining the Grid Search Functions to tune the Hyperparameters of the models:\n","---\n","Hyperparameter tuning is a critical step in the machine learning pipeline to optimize model performance. GridSearch is a method that systematically searches through a specified hyperparameter space to find the combination of hyperparameters that yields the best model performance.\n","\n","## Steps for Using GridSearch:\n","\n","1. **Define Hyperparameter Space**: Identify the hyperparameters you want to tune and specify the range or values for each hyperparameter.\n","\n","2. **Select Estimator**: Choose the machine learning algorithm you want to use and create an instance of it.\n","\n","3. **Create GridSearchCV Object**: Instantiate the `GridSearchCV` class from the scikit-learn library. Provide the estimator, hyperparameter grid, cross-validation strategy, and other necessary parameters.\n","\n","4. **Fit GridSearchCV**: Fit the `GridSearchCV` object to your training data. This will perform an exhaustive search over the hyperparameter space, evaluating each combination using cross-validation.\n","\n","5. **Access Best Estimator and Parameters**: After the search is complete, access the best estimator (model with the best hyperparameters) and the associated hyperparameters.\n","\n","6. **Evaluate Model**: Evaluate the best model on a separate validation set or test set to get an accurate estimate of its performance.\n","\n","Using GridSearch, you can efficiently explore the hyperparameter space and identify the best configuration for your machine learning model, leading to improved model performance.\n"]},{"cell_type":"markdown","metadata":{"id":"fe4e7d1c"},"source":["## Multinomial Naive Bayes Classifier with Hyperparameter Tuning\n","\n","This function fits a Multinomial Naive Bayes classifier using grid search for hyperparameter tuning.\n","\n","## Function: `Fit_Multinomial_Grid`\n","\n","### Parameters\n","\n","- **`training_set`** (*array-like*): The training data.\n","- **`training_classifications`** (*array-like*): The corresponding class labels.\n","\n","### Returns\n","\n","- **`optimised`**: The optimized Multinomial Naive Bayes classifier.\n","\n","### Procedure\n","\n","1. **Define Hyperparameters**: Prepare a list of hyperparameter values to be explored during grid search. In this case, the hyperparameter is `'alpha'`.\n","\n","2. **Create GridSearchCV Object**: Construct a GridSearchCV object using the MultinomialNB estimator. Set the parameter grid as the defined hyperparameter values, the number of cross-validation folds (cv), and configure error handling.\n","\n","3. **Fit Grid Search**: Fit the grid search to the training data. The best estimator is determined by cross-validation performance.\n","\n","4. **Retrieve Optimized Estimator**: Retrieve the best estimator (optimized classifier) from the grid search results.\n","\n","5. **Display Optimized Parameters**: Print the optimized parameters obtained from the grid search.\n","\n","6. **Return Optimized Estimator**: Return the optimized Multinomial Naive Bayes classifier.\n","\n","---\n","\n","By using this function, you can automatically fine-tune hyperparameters for a Multinomial Naive Bayes classifier using grid search. The grid search process explores various `'alpha'` values, helping to optimize the classifier's performance on the training data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4659ad5f"},"outputs":[],"source":["def Fit_Multinomial_Grid(training_set, training_classifications):\n","    \"\"\"\n","    Fit a Multinomial Naive Bayes classifier using grid search for hyperparameter tuning.\n","\n","    Args:\n","        training_set (array-like): The training data.\n","        training_classifications (array-like): The corresponding class labels.\n","\n","    Returns:\n","        optimised: The optimized Multinomial Naive Bayes classifier.\n","    \"\"\"\n","    # Define hyperparameter values to search\n","    parameters = {'alpha': [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50]}\n","\n","    # Create a GridSearchCV object with MultinomialNB estimator\n","    clf = GridSearchCV(estimator=naive_bayes.MultinomialNB(),\n","                       param_grid=parameters,\n","                       cv=3,\n","                       refit=True,\n","                       error_score=0,\n","                       n_jobs=-1)\n","\n","    # Fit the grid search to the training data\n","    clf.fit(training_set, training_classifications)\n","\n","    # Get the best estimator (optimized classifier)\n","    optimised = clf.best_estimator_\n","    print(\"Optimised Parameters from grid search: \",optimised)\n","\n","    return optimised\n"]},{"cell_type":"markdown","metadata":{"id":"97909388"},"source":["## Logistic Regression Classifier with Hyperparameter Tuning\n","\n","This function fits a Logistic Regression classifier using grid search for hyperparameter tuning.\n","\n","## Function: `Fit_Logistic_Regression_Grid`\n","\n","### Parameters\n","\n","- **`training_set`** (*array-like*): The training data.\n","- **`training_classifications`** (*array-like*): The corresponding class labels.\n","\n","### Returns\n","\n","- **`optimised`**: The optimized Logistic Regression classifier.\n","\n","### Procedure\n","\n","1. **Define Hyperparameters**: Prepare a list of hyperparameter values to be explored during grid search. In this case, the hyperparameter is `'C'`.\n","\n","2. **Create GridSearchCV Object**: Construct a GridSearchCV object using the LogisticRegression estimator. Set the parameter grid as the defined hyperparameter values, the number of cross-validation folds (cv), configure error handling, and specify `'accuracy'` as the scoring metric.\n","\n","3. **Fit Grid Search**: Fit the grid search to the training data. The best estimator is determined by cross-validation performance.\n","\n","4. **Retrieve Optimized Estimator**: Retrieve the best estimator (optimized classifier) from the grid search results.\n","\n","5. **Display Optimized Parameters**: Print the optimized parameters obtained from the grid search.\n","\n","6. **Return Optimized Estimator**: Return the optimized Logistic Regression classifier.\n","\n","---\n","\n","By using this function, you can automatically fine-tune hyperparameters for a Logistic Regression classifier using grid search. The grid search process explores various `'C'` values and leverages class weights for balancing, helping to optimize the classifier's performance on the training data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"438e5daa"},"outputs":[],"source":["def Fit_Logistic_Regression_Grid(training_set, training_classifications):\n","    \"\"\"\n","    Fit a Logistic Regression classifier using grid search for hyperparameter tuning.\n","\n","    Args:\n","        training_set (array-like): The training data.\n","        training_classifications (array-like): The corresponding class labels.\n","\n","    Returns:\n","        optomised: The optimized Logistic Regressor classifier.\n","    \"\"\"\n","    # Define hyperparameter values to search\n","    parameters = {'C': [ 0.01,0.05, 0.1,0.5,1,5,10]}\n","\n","    # Create a GridSearchCV object with Logistic Regression estimator\n","    clf = GridSearchCV(estimator = LogisticRegression(class_weight = 'balanced', penalty = 'l2', max_iter=10000),\n","                       param_grid=parameters,\n","                       cv=3,\n","                       refit=True,\n","                       scoring = 'accuracy',\n","                       n_jobs=-1)\n","\n","    # Fit the grid search to the training data\n","    clf.fit(training_set, training_classifications)\n","\n","    # Get the best estimator (optimized classifier)\n","    optimised = clf.best_estimator_\n","    print(\"Optimised Parameters from grid search: \",optimised)\n","\n","    return optimised\n"]},{"cell_type":"markdown","metadata":{"id":"d66b577d"},"source":["## One-vs-Rest Logistic Regression Classifier with Hyperparameter Tuning\n","\n","This function fits a One-vs-Rest Logistic Regression classifier using grid search for hyperparameter tuning.\n","\n","## Function: `Fit_One_VS_Rest_Log_Regression_Grid`\n","\n","### Parameters\n","\n","- **`training_set`** (*array-like*): The training data.\n","- **`training_classifications`** (*array-like*): The corresponding class labels.\n","\n","### Returns\n","\n","- **`optimised`**: The optimized One-vs-Rest Logistic Regression classifier.\n","\n","### Procedure\n","\n","1. **Define Hyperparameters**: Prepare a list of hyperparameter values to be explored during grid search. In this case, the hyperparameter is `'estimator__alpha'`.\n","\n","2. **Create One-vs-Rest Model**: Construct a OneVsRestClassifier model using the SGDClassifier with `'log_loss'` loss and `'l1'` penalty.\n","\n","3. **Create GridSearchCV Object**: Build a GridSearchCV object using the OneVsRest model. Set the parameter grid as the defined hyperparameter values, the number of cross-validation folds (cv), configure error handling, and specify `'accuracy'` as the scoring metric.\n","\n","4. **Fit Grid Search**: Fit the grid search to the training data. The best estimator is determined by cross-validation performance.\n","\n","5. **Retrieve Optimized Estimator**: Retrieve the best estimator (optimized classifier) from the grid search results.\n","\n","6. **Display Optimized Parameters**: Print the optimized parameters obtained from the grid search.\n","\n","7. **Return Optimized Estimator**: Return the optimized One-vs-Rest Logistic Regression classifier.\n","\n","---\n","\n","By using this function, you can automatically fine-tune hyperparameters for a One-vs-Rest Logistic Regression classifier using grid search. The grid search process explores various `'estimator__alpha'` values and leverages the One\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a3dcdcb7"},"outputs":[],"source":["def Fit_One_VS_Rest_Log_Regression_Grid(training_set, training_classifications):\n","    \"\"\"\n","    Fit a OneVsRest Logistic classifier using grid search for hyperparameter tuning.\n","\n","    Args:\n","        training_set (array-like): The training data.\n","        training_classifications (array-like): The corresponding class labels.\n","\n","    Returns:\n","        optimised: The optimized OneVSRest classifier.\n","    \"\"\"\n","    # Define hyperparameter values to search\n","    parameters  = {\"estimator__alpha\": [10**-5, 10**-3, 10**-1, 10**1, 10**2]}\n","\n","    model = OneVsRestClassifier(SGDClassifier(loss='log_loss',penalty='l1'))\n","\n","    # Create a GridSearchCV object with MultinomialNB estimator\n","    clf = GridSearchCV(model,\n","                       param_grid=parameters,\n","                       cv=3,\n","                       refit=True,\n","                       scoring = 'accuracy',\n","                       n_jobs=-1)\n","\n","    # Fit the grid search to the training data\n","    clf.fit(training_set, training_classifications)\n","\n","    # Get the best estimator (optimized classifier)\n","    optimised = clf.best_estimator_\n","    print(\"Optimised Parameters from grid search: \",optimised)\n","\n","    return optimised\n"]},{"cell_type":"markdown","metadata":{"id":"647d4729"},"source":["# Grid Search for Support Vector Machine (SVM) Model Optimization\n","\n","This function performs a grid search to optimize hyperparameters for a Support Vector Machine (SVM) classification model. Grid search is a technique used to systematically search through a specified range of hyperparameters to find the combination that produces the best performance.\n","\n","## Function: `Fit_SVM_Grid`\n","\n","### Parameters\n","\n","- **`training_set`** (*array-like*): The training dataset features.\n","- **`training_classifications`** (*array-like*): The corresponding class labels for the training dataset.\n","\n","### Returns\n","\n","- **`optimised`** (*SVC*): The optimized SVM classifier with the best hyperparameters.\n","\n","---\n","\n","Using this function, you can efficiently search through different combinations of hyperparameters to find the configuration that results in the highest accuracy for your SVM classification model. The example usage below demonstrates how to call the function with the training set and classifications to obtain the optimized SVM classifier.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02c8abf4"},"outputs":[],"source":["def Fit_SVM_Grid(training_set, training_classifications):\n","\n","# Define hyperparameter values to search\n","\n","    parameters = {'C': [1, 10], 'gamma': [0.001, 0.01, 1]}\n","\n","    model = SVC()\n","\n","    # Create a GridSearchCV object with MultinomialNB estimator\n","    clf = GridSearchCV(model,\n","                       param_grid=parameters,\n","                       cv=3,\n","                       refit=True,\n","                       scoring = 'accuracy',\n","                       n_jobs=-1)\n","\n","    # Fit the grid search to the training data\n","    clf.fit(training_set, training_classifications)\n","\n","    # Get the best estimator (optimized classifier)\n","    optimised = clf.best_estimator_\n","    print(\"Optimised Parameters from grid search: \",optimised)\n","\n","    return optimised"]},{"cell_type":"markdown","metadata":{"id":"ba1dce23"},"source":["# Grid Search for Support Vector Machine (SVM) Model Optimization\n","\n","This function performs a grid search to optimize hyperparameters for a Support Vector Machine (SVM) classification model. Grid search is a technique used to systematically search through a specified range of hyperparameters to find the combination that produces the best performance.\n","\n","## Function: `Fit_SVM_Grid`\n","\n","### Parameters\n","\n","- **`training_set`** (*array-like*): The training dataset features.\n","- **`training_classifications`** (*array-like*): The corresponding class labels for the training dataset.\n","\n","### Returns\n","\n","- **`optimised`** (*SVC*): The optimized SVM classifier with the best hyperparameters.\n","\n","---\n","\n","Using this function, you can efficiently search through different combinations of hyperparameters to find the configuration that results in the highest accuracy for your SVM classification model. The example usage below demonstrates how to call the function with the training set and classifications to obtain the optimized SVM classifier.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"495827f1"},"outputs":[],"source":["def Fit_Random_Forest_Grid(training_set, training_classifications):\n","\n","    # Define hyperparameter values to search\n","\n","    parameters = {'n_estimators': [1,5,10],\n","              'max_depth': [1,3,5]}\n","\n","    model = RandomForestClassifier(criterion = 'entropy')\n","\n","    # Create a GridSearchCV object with MultinomialNB estimator\n","    clf = GridSearchCV(model,\n","                       param_grid=parameters,\n","                       cv=3,\n","                       refit=True,\n","                       scoring = 'accuracy',\n","                       n_jobs=-1)\n","\n","    # Fit the grid search to the training data\n","    clf.fit(training_set, training_classifications)\n","\n","    # Get the best estimator (optimized classifier)\n","    optimised = clf.best_estimator_\n","    print(\"Optimised Parameters from grid search: \",optimised)\n","\n","    return optimised\n"]},{"cell_type":"markdown","metadata":{"id":"3134b7c4"},"source":["## Fit the Bag of Words (BoW) Classification Model\n","\n","This function fits a classification model using the Bag of Words (BoW) approach with the option to choose between Logistic Regression, Multinomial Naive Bayes, or One-vs-Rest classifiers. It also evaluates the model and displays evaluation metrics, a confusion matrix, ROC curves, and precision-recall curves.\n","\n","## Function: `fit_BoW_model`\n","\n","### Parameters\n","\n","- **`X`** (*pandas.DataFrame*): The input DataFrame containing feature data.\n","- **`y`** (*np.array*): The label-encoded categorical target.\n","- **`field`** (*str*): The name of the column in the DataFrame containing the text data.\n","- **`target`** (*str*): The name of the column in the DataFrame containing the target variable.\n","- **`model_name`** (*str*): The name of the classification model to use.\n","\n","### Returns\n","\n","- **None** (Prints evaluation metrics and plots confusion matrix, ROC curve, and precision-recall curve.)\n","\n","### Procedure\n","\n","1. **Split Training and Test Sets**: Split the DataFrame into training and test sets.\n","\n","2. **TF-IDF Vectorization**: Utilize the TfidfVectorizer to convert the training corpus into TF-IDF features.\n","\n","3. **Feature Selection**: Perform feature selection using the Chi-squared test to select relevant features for each category.\n","\n","4. **Re-fit Vectorizer**: Re-fit the vectorizer with the selected features.\n","\n","5. **Choose Classifier**: Depending on the specified *model_name*, choose either the Logistic Regression, Multinomial Naive Bayes, or One-vs-Rest classifier using respective functions.\n","\n","6. **Train the Classifier**: Train the chosen classifier using a pipeline with the vectorizer.\n","\n","7. **Test the Model**: Test the trained model on the test data and calculate predicted probabilities.\n","\n","8. **Evaluate Model**:\n","   - Calculate accuracy, AUC, and display a classification report.\n","   - Plot a confusion matrix.\n","   - Plot ROC curves for each class.\n","   - Plot precision-recall curves for each class.\n","\n","---\n","\n","By utilizing this function, you can fit and evaluate a classification model using the Bag of Words (BoW) approach. The function offers flexibility in choosing the classification model and provides a comprehensive evaluation of the model's performance, aiding in assessing its effectiveness on the given data.\n","\n","\n","Note: The 'CLEAN_TEXT' column contains the preprocessed text data, and the 'CATEGORY' column contains the target variable with different categories.\n","\n","The accuracy plot explores the model's performance in terms of accuracy, precision, recall, and the area under the ROC curve for each class. The confusion matrix provides insights into the model's classification performance by showing the number of true positives, false positives, true negatives, and false negatives for each class. The ROC curve represents the trade-off between the true positive rate (recall) and the false positive rate, and the precision-recall curve shows the trade-off between precision and recall for each class. These plots help assess the model's ability to correctly classify instances of each category and identify the trade-offs between different evaluation metrics.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c01f00ca"},"outputs":[],"source":["\n","# Fit the BoW classification model:\n","\n","def fit_BoW_model(X, y, field, target, model_name):\n","    \"\"\"\n","    Fit the classification model using the Bag of Words (BoW) approach with the Multinomial Naive Bayes classifier.\n","\n","    Args:\n","        X (pandas.DataFrame): The input DataFrame containing Feature Data.\n","        y (np.array): The Label Encoded Categorical Target.\n","        field (str): The name of the column in the DataFrame containing the text data.\n","        target (str): The name of the column in the DataFrame containing the target variable.\n","        model (str) :\n","\n","    Returns:\n","        None (Prints evaluation metrics and plots confusion matrix, ROC curve, and precision-recall curve.)\n","    \"\"\"\n","\n","\n","    # Split the DataFrame into training and test sets\n","    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n","\n","    ## TF-IDF (advanced variant of BoW)\n","    vectorizer = feature_extraction.text.TfidfVectorizer(max_features=20000, ngram_range=(1,2))\n","\n","    # Fit the vectorizer on the training corpus\n","    corpus = X_train[field]\n","    vectorizer.fit(corpus)\n","    X_train = vectorizer.transform(corpus)\n","    print(\"TF-IDF Vectorization Shape:\", X_train.shape)\n","    dic_vocabulary = vectorizer.vocabulary_\n","\n","    # Feature Selection:\n","    X_names = vectorizer.get_feature_names_out()\n","    p_value_limit = 0.95\n","    df_features = pd.DataFrame()\n","    for cat in np.unique(y_train):\n","        # Perform Chi-squared test for feature selection\n","        chi2, p = feature_selection.chi2(X_train, y_train==cat)\n","        df_features = pd.concat([df_features,pd.DataFrame(\n","                       {'feature':X_names, 'score':1-p, 'y':cat})])\n","        df_features = df_features.sort_values(['y','score'],\n","                        ascending=[True,False])\n","        df_features = df_features[df_features['score']>p_value_limit]\n","    X_names = df_features['feature'].unique().tolist()\n","    print(\"Selected Features Count:\", len(X_names))\n","\n","    # Show most relevant vectors in each category:\n","    for cat in np.unique(y_train):\n","        print(f\"# {cat}:\")\n","        print(f\"  . Selected Features: {len(df_features[df_features['y']==cat])}\")\n","        print(f\"  . Top Features: {', '.join(df_features[df_features['y']==cat]['feature'].values[:10])}\")\n","        print()\n","\n","    # Re-fit the vectorizer with the new selected features:\n","    vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)\n","    vectorizer.fit(corpus)\n","    X_train = vectorizer.transform(corpus)\n","    dic_vocabulary = vectorizer.vocabulary_\n","\n","    # Choose the classifier model (Naive Bayes)\n","    classifier =''\n","    if model_name == 'Logistic Regression':\n","        classifier = Fit_Logistic_Regression_Grid(X_train, y_train)\n","    elif model_name == 'Multinomial Naive Bayes':\n","        classifier = Fit_Multinomial_Grid(X_train, y_train)\n","    elif model_name == 'OneVsRest':\n","        classifier = Fit_One_VS_Rest_Log_Regression_Grid(X_train, y_train)\n","    elif model_name == 'SVM':\n","        classifier = Fit_SVM_Grid(X_train, y_train)\n","    elif model_name == 'Random Forest':\n","        classifier = Fit_Random_Forest_Grid(X_train, y_train)\n","    else:\n","        print(\"No model name recognised, please refer to help function for function to ensure proper parameter is used.\")\n","        return\n","\n","    # Train the Classifier using a pipeline\n","    model = pipeline.Pipeline([('vectorizer', vectorizer), ('classifier', classifier)])\n","    model['classifier'].fit(X_train, y_train)\n","\n","    # Test the model on the test data\n","    X_test = X_test[field].values\n","    predicted = model.predict(X_test)\n","    predicted_prob = model.predict_proba(X_test)\n","\n","    classes = np.unique(y_test)\n","    y_test_array = pd.get_dummies(y_test, drop_first=False).values\n","\n","    ## Calculate Accuracy, Precision, Recall, and AUC\n","    accuracy = metrics.accuracy_score(y_test, predicted)\n","    auc = metrics.roc_auc_score(y_test, predicted_prob, multi_class=\"ovr\")\n","\n","    print(\"Accuracy:\", round(accuracy,2))\n","    print(\"AUC:\", round(auc,2))\n","    print(\"Classification Report:\")\n","    print(metrics.classification_report(y_test, predicted))\n","\n","    ## Plot Confusion Matrix\n","    cm = metrics.confusion_matrix(y_test, predicted)\n","    fig, ax = plt.subplots()\n","    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n","    ax.set(xlabel=\"Predicted\", ylabel=\"True\", xticklabels=classes, yticklabels=classes, title=\"Confusion Matrix\")\n","    plt.yticks(rotation=0)\n","\n","    fig, ax = plt.subplots(nrows=1, ncols=2)\n","    # Plot ROC\n","    for i in range(len(classes)):\n","        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i], predicted_prob[:, i])\n","        ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n","        ax[0].set(xlim=[-0.05, 1.0], ylim=[0.05, 1.05],\n","                xlabel=\"False Positive Rate\",\n","                ylabel=\"True Positive Rate (Recall)\",\n","                title=\"Receiver Operating Characteristic (ROC) Curve\")\n","        ax[0].legend(loc=\"lower right\")\n","        ax[0].grid(True)\n","\n","    # Plot Precision-Recall Curve\n","    for i in range(len(classes)):\n","        precision, recall, thresholds = metrics.precision_recall_curve(y_test_array[:,i], predicted_prob[:,i])\n","        ax[1].plot(recall, precision, lw=3, label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(recall, precision)))\n","        ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', ylabel=\"Precision\", title=\"Precision-Recall Curve\")\n","        ax[1].legend(loc=\"best\")\n","        ax[1].grid(True)\n","\n","    box = ax[1].get_position()\n","    box.x0 = box.x0 + 0.1\n","    box.x1 = box.x1 + 0.1\n","    ax[1].set_position(box)\n","    plt.show()\n","\n","# Example usage of the function\n","\n","#fit_BoW_model(X, y, 'CLEAN_TEXT', 'CATEGORY','SVM')"]},{"cell_type":"markdown","metadata":{"id":"cf98b4ee"},"source":["# Model Comparisons\n","\n","### Multinomial Naive Bayes\n","\n","**Optimised Parameters from grid search:**  \n","MultinomialNB(alpha=0.1)\n","\n","**Accuracy:** 0.92 | **AUC:** 0.99\n","\n","|           | Precision | Recall  | F1-Score | Support |\n","|-----------|-----------|---------|----------|---------|\n","| Class 0   | 0.89      | 0.90    | 0.89     | 23142   |\n","| Class 1   | 0.95      | 0.96    | 0.96     | 30407   |\n","| Class 2   | 0.95      | 0.87    | 0.91     | 9160    |\n","| Class 3   | 0.89      | 0.89    | 0.89     | 21775   |\n","| **Average** | **0.92**  | **0.92** | **0.92** | **84484** |\n","\n","**Macro Avg:** Precision: 0.92 | Recall: 0.91 | F1-Score: 0.91  \n","\n","**Weighted Avg:** Precision: 0.92 | Recall: 0.92 | F1-Score: 0.92 | Support: 84484\n","\n","---\n","\n","### Logistic Regression\n","\n","**Optimised Parameters from grid search:**  \n","LogisticRegression(C=5, class_weight='balanced', max_iter=10000)\n","\n","**Accuracy:** 0.94 | **AUC:** 0.99\n","\n","|           | Precision | Recall  | F1-Score | Support |\n","|-----------|-----------|---------|----------|---------|\n","| Class 0   | 0.92      | 0.91    | 0.91     | 23170   |\n","| Class 1   | 0.97      | 0.97    | 0.97     | 30631   |\n","| Class 2   | 0.89      | 0.94    | 0.92     | 9122    |\n","| Class 3   | 0.92      | 0.91    | 0.92     | 21561   |\n","| **Average** | **0.93**  | **0.94** | **0.93** | **84484** |\n","\n","**Macro Avg:** Precision: 0.92 | Recall: 0.93 | F1-Score: 0.93  \n","\n","**Weighted Avg:** Precision: 0.94 | Recall: 0.94 | F1-Score: 0.94 | Support: 84484\n","\n","---\n","\n","### One vs Rest Logistic Regression\n","\n","**Optimised Parameters from grid search:**  \n","OneVsRestClassifier(estimator=SGDClassifier(alpha=1e-05, loss='log_loss', penalty='l1'))\n","\n","**Accuracy:** 0.93 | **AUC:** 0.99\n","\n","|           | Precision | Recall  | F1-Score | Support |\n","|-----------|-----------|---------|----------|---------|\n","| Class 0   | 0.90      | 0.91    | 0.90     | 23237   |\n","| Class 1   | 0.94      | 0.97    | 0.96     | 30408   |\n","| Class 2   | 0.95      | 0.88    | 0.91     | 9158    |\n","| Class 3   | 0.91      | 0.90    | 0.91     | 21681   |\n","| **Average** | **0.93**  | **0.93** | **0.92** | **84484** |\n","\n","**Macro Avg:** Precision: 0.93 | Recall: 0.91 | F1-Score: 0.92  \n","\n","**Weighted Avg:** Precision: 0.93 | Recall: 0.93 | F1-Score: 0.92 | Support: 84484\n"]},{"cell_type":"markdown","metadata":{"id":"95e18e53"},"source":["---\n","# Introduction to Word Embeddings with Word2Vec\n","---\n","\n","Word embedding is a powerful method for representing the vocabulary of a document. It captures various aspects of words such as context, similarity, and relationships. Word embeddings are essentially vector representations of words, and they are generated using techniques like Word2Vec.\n","\n","## What are Word Embeddings?\n","\n","Word embeddings are vector representations of words. They capture word meaning, context, and syntactic relationships. Word2Vec is a popular technique for learning word embeddings through a shallow neural network. Developed by Tomas Mikolov at Google in 2013, Word2Vec is widely used for this purpose.\n","\n","## The Need for Word Embeddings\n","\n","Consider similar sentences: \"Have a good day\" and \"Have a great day.\" Despite minor differences, their meanings are quite similar. If we create a vocabulary V = {Have, a, good, great, day}, and encode each word with a one-hot vector, they would look like this:\n","\n","Have = [1,0,0,0,0]\n","a = [0,1,0,0,0]\n","good = [0,0,1,0,0]\n","great = [0,0,0,1,0]\n","day = [0,0,0,0,1]\n","\n","Visualizing this, each word occupies one dimension, and the rest are irrelevant. This means 'good' and 'great' are as different as 'day' and 'have', which isn't accurate.\n","\n","The goal is to position words with similar context close in space. Mathematically, their cosine similarity should be close to 1, indicating a small angle between vectors.\n","\n","## Word2Vec and Contextual Similarity\n","\n","Word2Vec uses neural networks to learn word embeddings. It captures context by training on a corpus of text. The model learns to predict context words given a target word. As a result, words with similar contexts end up closer in the vector space.\n","\n","In summary, word embeddings provide richer word representations than one-hot encodings. Word2Vec is a popular technique for learning embeddings, allowing words with similar meanings and contexts to be represented as vectors close in space.\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"de8d54dd"},"source":["# Load Google's Pre-trained Word2Vec Model\n","\n","This function loads the Word2Vec model trained on a Google News corpus. Google's pre-trained Word2Vec model contains word embeddings learned from a vast amount of text data from the Google News dataset. These embeddings capture semantic relationships between words, enabling various natural language processing tasks.\n","\n","## Function: `load_Google_word2vec`\n","\n","### Returns\n","\n","- **`model`** (*gensim.models.KeyedVectors*): The loaded Word2Vec model.\n","\n","### Procedure\n","\n","1. **Load Model**: The function loads the Word2Vec model using the `gensim.models.KeyedVectors.load_word2vec_format` method. This method loads the model's binary format.\n","\n","2. **Model Source**: The model is trained on an extensive Google corpus, making it a valuable resource for understanding word semantics.\n","\n","By using this function, you can easily load Google's pre-trained Word2Vec model and utilize the pre-learned word embeddings for various natural language processing tasks. The example provided at the end demonstrates how to call the function and load the model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f03b0342"},"outputs":[],"source":["def load_Google_word2vec():\n","\n","    \"\"\"\n","    Load the Word2Vec model trained on a Google News corpus.\n","\n","    Returns:\n","        gensim.models.KeyedVectors: The loaded Word2Vec model.\n","\n","    \"\"\"\n","\n","    # Load Word2Vec model (trained on an enormous Google corpus)\n","    model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary = True)\n","    return model\n","\n","#model = load_Google_word2vec()"]},{"cell_type":"markdown","metadata":{"id":"f8a90830"},"source":["## Visualize Word Vectors using t-SNE Dimensionality Reduction\n","\n","This function visualizes word vectors using the t-SNE (t-Distributed Stochastic Neighbor Embedding) dimensionality reduction technique. It takes a DataFrame containing text data, a specific column from the DataFrame, and a pre-trained word vectors model as inputs. The t-SNE visualization is used to display word vectors in a two-dimensional space while preserving their similarity relationships.\n","\n","## Function: `show_vectors_tsne`\n","\n","### Parameters\n","\n","- **`df`** (*pandas.DataFrame*): The DataFrame containing the text data.\n","- **`field`** (*str*): The name of the column in the DataFrame containing the text data.\n","- **`model`** (*gensim.models.keyedvectors.Word2VecKeyedVectors*): The pre-trained word vectors model.\n","\n","### Returns\n","\n","- **None** (Displays t-SNE visualization of word vectors).\n","\n","### Procedure\n","\n","1. **Form Corpus List**: Generate a list of words from the specified column of the DataFrame.\n","2. **Filter Words**: Filter out words that are not present in the model's vocabulary.\n","3. **Combine Vectors and Words**: Create a dictionary that maps filtered words to their corresponding word vectors.\n","4. **Create DataFrame**: Convert the dictionary to a DataFrame for further processing.\n","5. **t-SNE Dimensionality Reduction**: Use t-SNE to reduce the dimensionality of word vectors to two dimensions.\n","6. **Plot Visualization**: Plot a scatterplot using Seaborn to visualize the reduced word vectors.\n","7. **Add Labels to Points**: Add labels to some points on the plot using the `adjust_text` function to handle overlapping labels.\n","\n","By using this function, you can visually explore the relationships between word vectors in a two-dimensional space, gaining insights into word similarities and clusters. The example usage provided at the end demonstrates how to call the function with appropriate arguments.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6703e865"},"outputs":[],"source":["def show_vectors_tsne(df, field, model):\n","    \"\"\"\n","    Visualize word vectors using t-SNE dimensionality reduction technique.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame containing the text data.\n","        field (str): The name of the column in the DataFrame containing the text data.\n","        model (gensim.models.keyedvectors.Word2VecKeyedVectors): The pre-trained word vectors model.\n","\n","    Returns:\n","        None (Displays t-SNE visualization of word vectors).\n","    \"\"\"\n","\n","    # Form the list of corpus from the specified field\n","    corpus = form_corpora_list(df, field, False)\n","\n","    # Filter out words not present in the model's vocabulary\n","    vector_list = [model[word] for word in corpus if word in model.key_to_index]\n","    words_filtered = [word for word in corpus if word in model.key_to_index]\n","\n","    # Combine words and their vectors into a dictionary\n","    word_vec_zip = zip(words_filtered, vector_list)\n","    word_vec_dict = dict(word_vec_zip)\n","\n","    # Convert the dictionary to a DataFrame\n","    df_word_vec = pd.DataFrame.from_dict(word_vec_dict, orient='index')\n","\n","    # Perform t-SNE dimensionality reduction\n","    tsne = TSNE(n_components=2, init='random', random_state=10, perplexity=100)\n","    tsne_df = tsne.fit_transform(df_word_vec[:400])\n","\n","    # Plot the t-SNE visualization\n","    sns.set()\n","    fig, ax = plt.subplots(figsize=(11.7, 8.27))\n","    sns.scatterplot(x=tsne_df[:, 0], y=tsne_df[:, 1], alpha=0.5)\n","\n","    # Add labels to some points using adjustText to handle overlapping text\n","    texts = []\n","    words_to_plot = list(np.arange(0, 400, 10))\n","    for word in words_to_plot:\n","        texts.append(plt.text(tsne_df[word, 0], tsne_df[word, 1], df_word_vec.index[word], fontsize=14))\n","\n","    adjust_text(texts, force_points=0.4, force_text=0.4, expand_points=(2, 1), expand_text=(1, 2),\n","                arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n","\n","    plt.show()\n","\n","# Example usage\n","#show_vectors_tsne(df, 'CLEAN_TEXT', model)\n"]},{"cell_type":"markdown","metadata":{"id":"5ea97aa6"},"source":["# Document Vector Generation using Word Vectors\n","\n","This function generates a document vector by calculating the mean of word vectors for words in a given document. It utilizes pre-trained word vectors from a Word2Vec model to represent the document in a continuous vector space.\n","\n","## Function: `document_vector`\n","\n","### Parameters\n","\n","- **`word2vec_model`** (*gensim.models.keyedvectors.Word2VecKeyedVectors*): The pre-trained word vectors model.\n","- **`doc`** (*list*): List of words in the document.\n","\n","### Returns\n","\n","- **`numpy.ndarray`**: A document vector representing the mean of word vectors in the document. If no words are found in the model, it returns a vector of zeros.\n","\n","### Procedure\n","\n","1. **Filtering Out-of-Vocabulary Words**: Remove words from the document that are not present in the Word2Vec model's vocabulary.\n","2. **Calculating Mean Vector**: Calculate the mean vector of word vectors for the remaining words in the document using `np.mean` along the specified axis (axis=0).\n","3. **Return Document Vector**: Return the generated document vector.\n","\n","Using this function, you can transform a document into a continuous vector representation using the mean of word vectors. This representation can be useful for various natural language processing tasks, such as document classification, clustering, and similarity calculations. The example usage below demonstrates how to call the function with a Word2Vec model and a document to obtain its vector representation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c89a689b"},"outputs":[],"source":["def document_vector(word2vec_model, doc):\n","\n","    \"\"\"\n","    Generate a document vector by calculating the mean of word vectors in the given document.\n","\n","    Args:\n","        word2vec_model (gensim.models.keyedvectors.Word2VecKeyedVectors): The pre-trained word vectors model.\n","        doc (list): List of words in the document.\n","\n","    Returns:\n","        numpy.ndarray: A document vector representing the mean of word vectors in the document.\n","                       If no words are found in the model, returns a vector of zeros.\n","    \"\"\"\n","\n","    # remove out-of-vocabulary words\n","    doc = [word for word in doc if word in model.key_to_index]\n","    return np.mean(model[doc], axis=0)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f590f0e3"},"source":["# Document Vector Representation Check using Word Vectors\n","\n","This function checks whether a document has a vector representation in a given Word2Vec model's vocabulary. It helps identify documents that contain only out-of-vocabulary words and, therefore, cannot be represented by word vectors.\n","\n","## Function: `has_vector_representation`\n","\n","### Parameters\n","\n","- **`word2vec_model`** (*gensim.models.keyedvectors.Word2VecKeyedVectors*): The pre-trained word vectors model.\n","- **`doc`** (*list*): List of words in the document.\n","\n","### Returns\n","\n","- **`bool`**: True if the document has a vector representation in the model, False otherwise.\n","\n","### Procedure\n","\n","1. **Checking Vector Representation**: For each word in the document, the function checks if the word is present in the Word2Vec model's vocabulary using the `key_to_index` attribute. If all words in the document are not found in the model's vocabulary, the function returns False; otherwise, it returns True.\n","\n","This function can be used to filter out documents that cannot be represented using the provided Word2Vec model's vocabulary. By applying this function before document vector generation, you can ensure that only valid documents are used for further analysis and processing. The example usage below demonstrates how to call the function with a Word2Vec model and a document to check its vector representation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"db73d9b7"},"outputs":[],"source":["# Function that will help us drop documents that have no word vectors in word2vec\n","def has_vector_representation(word2vec_model, doc):\n","\n","    \"\"\"\n","    Check if a document has vector representation in the given word vectors model.\n","\n","    Args:\n","        word2vec_model (gensim.models.keyedvectors.Word2VecKeyedVectors): The pre-trained word vectors model.\n","        doc (list): List of words in the document.\n","\n","    Returns:\n","        bool: True if the document has vector representation, False otherwise.\n","    \"\"\"\n","\n","\n","    return not all(word not in word2vec_model.key_to_index for word in doc)"]},{"cell_type":"markdown","metadata":{"id":"b335adc6"},"source":["# Document Filtering based on a Condition\n","\n","This function filters out documents from a corpus based on a given condition. It allows for efficient removal of unwanted documents, helping to clean and refine the dataset before further analysis.\n","\n","## Function: `filter_docs`\n","\n","### Parameters\n","\n","- **`corpus`** (*list*): List of documents.\n","- **`texts`** (*list*): List of text data corresponding to each document.\n","- **`condition_on_doc`** (*function*): A function that takes a document and returns a boolean indicating whether the document should be retained or removed.\n","\n","### Returns\n","\n","- **`tuple`**: A tuple containing the filtered corpus and filtered texts (if provided).\n","\n","### Procedure\n","\n","1. **Initial Corpus Size**: Calculate the initial number of documents in the corpus.\n","\n","2. **Filtering Texts (if provided)**: If a list of texts is provided, iterate through each text and corresponding document in the corpus. For each pair, apply the `condition_on_doc` function to check whether the document should be retained or removed. If the condition is met, keep the text; otherwise, discard it.\n","\n","3. **Filtering Corpus**: Iterate through the entire corpus and retain only the documents that satisfy the condition specified by the `condition_on_doc` function.\n","\n","4. **Display Removed Count**: Calculate the difference between the initial number of documents and the final number of documents in the filtered corpus. Print the count of removed documents.\n","\n","5. **Return Filtered Data**: Return a tuple containing the filtered corpus and the filtered texts (if provided).\n","\n","By using this function, you can easily filter out documents from a corpus based on specific criteria. This is particularly useful for data preprocessing and cleaning tasks. The example usage below demonstrates how to apply the function with a condition and retrieve the filtered corpus and texts (if provided).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f455d249"},"outputs":[],"source":["# Filter out documents\n","def filter_docs(corpus, texts, condition_on_doc):\n","\n","    \"\"\"\n","    Filter out documents from a corpus based on a given condition.\n","\n","    Args:\n","        corpus (list): List of documents.\n","        texts (list): List of text data corresponding to each document.\n","        condition_on_doc (function): A function that takes a document and returns a boolean indicating whether the\n","                                    document should be retained or removed.\n","\n","    Returns:\n","        tuple: A tuple containing the filtered corpus and filtered texts (if provided).\n","    \"\"\"\n","\n","    number_of_docs = len(corpus)\n","\n","    if texts is not None:\n","        texts = [text for (text, doc) in zip(texts, corpus)\n","                 if condition_on_doc(doc)]\n","\n","    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n","\n","    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n","\n","    return (corpus, texts)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"53126c89"},"outputs":[],"source":["def Form_word2Vec_Xy(df, pre_clean_feature, clean_feature, target):\n","\n","    titles_list = [title for title in df[pre_clean_feature_feature]]\n","    corpus = [c_title for c_title in df[clean_feature]]\n","\n","    # Remove docs that don't include any words in W2V's vocab\n","    corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: has_vector_representation(model, doc))\n","\n","    # Filter out any empty docs\n","    corpus, titles_list = filter_docs(corpus, titles_list, lambda doc: (len(doc) != 0))\n","    # Initialize an array for the size of the corpus\n","    x = []\n","    for doc in corpus: # append the vector for each document\n","        x.append(document_vector(model, doc))\n","\n","    X = np.array(x) # list to array\n","\n","    from sklearn.decomposition import PCA\n","\n","    pca = PCA(n_components='mle', svd_solver='full', random_state=10)\n","\n","    # x is the array with our 300-dimensional vectors\n","    reduced_vecs = pca.fit_transform(x)\n","    df_w_vectors = pd.DataFrame(reduced_vecs)\n","\n","    df_w_vectors['Title'] = titles_list\n","\n","    main_w_vectors = pd.concat((df_w_vectors, df), axis=1)\n","\n","    # Get rid of vectors that couldn't be matched with the main_df\n","    main_w_vectors.dropna(axis=0, inplace=True)\n","\n","    # Drop all non-numeric, non-dummy columns, for feeding into the models\n","    cols_to_drop = ['Title','ID', 'URL', 'STORY', 'HOSTNAME', 'TIMESTAMP', 'PUBLISHER','TITLE', 'CLEAN_TEXT']\n","\n","    data = main_w_vectors.drop(columns=cols_to_drop, axis = 1)\n","\n","    X, y = Form_X_y(data, target)\n","\n","    return X, y\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7520ed35"},"outputs":[],"source":["# Fit the BoW classification model:\n","\n","def fit_Word2Vec_model(X, y, field, target, model_name):\n","\n","    \"\"\"\n","    Fit and evaluate a classification model using word embeddings (Word2Vec) on the given dataset.\n","\n","    Args:\n","        X (pandas.DataFrame): The DataFrame containing the input features.\n","        y (pandas.Series): The Series containing the target labels.\n","        field (str): The name of the column in the DataFrame containing the word embedding data.\n","        target (str): The name of the target column.\n","        model_name (str): The name of the classification model to be used.\n","\n","    Returns:\n","        None (Displays model evaluation metrics and visualization).\n","    \"\"\"\n","\n","    # Split the DataFrame into training and test sets\n","    X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n","\n","    # Choose the classifier model (Naive Bayes)\n","    classifier =''\n","    if model_name == 'Logistic Regression':\n","        classifier = Fit_Logistic_Regression_Grid(X_train, y_train)\n","    elif model_name == 'Multinomial Naive Bayes':\n","        classifier = Fit_Multinomial_Grid(X_train, y_train)\n","    elif model_name == 'OneVsRest':\n","        classifier = Fit_One_VS_Rest_Log_Regression_Grid(X_train, y_train)\n","    elif model_name == 'SVM':\n","        classifier = Fit_SVM_Grid(X_train, y_train)\n","    elif model_name == 'Random Forest':\n","        classifier = Fit_Random_Forest_Grid(X_train, y_train)\n","    else:\n","        print(\"No model name recognised, please refer to help function for function to ensure proper parameter is used.\")\n","        return\n","\n","    # Train the Classifier using a pipeline\n","    model = pipeline.Pipeline([('classifier', classifier)])\n","    model['classifier'].fit(X_train, y_train)\n","\n","    # Test the model on the test data\n","    predicted = model.predict(X_test)\n","    predicted_prob = model.predict_proba(X_test)\n","\n","    classes = np.unique(y_test)\n","    y_test_array = pd.get_dummies(y_test, drop_first=False).values\n","\n","    ## Calculate Accuracy, Precision, Recall, and AUC\n","    accuracy = metrics.accuracy_score(y_test, predicted)\n","    auc = metrics.roc_auc_score(y_test, predicted_prob, multi_class=\"ovr\")\n","\n","    print(\"Accuracy:\", round(accuracy,2))\n","    print(\"AUC:\", round(auc,2))\n","    print(\"Classification Report:\")\n","    print(metrics.classification_report(y_test, predicted))\n","\n","    ## Plot Confusion Matrix\n","    cm = metrics.confusion_matrix(y_test, predicted)\n","    fig, ax = plt.subplots()\n","    sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, cbar=False)\n","    ax.set(xlabel=\"Predicted\", ylabel=\"True\", xticklabels=classes, yticklabels=classes, title=\"Confusion Matrix\")\n","    plt.yticks(rotation=0)\n","\n","    fig, ax = plt.subplots(nrows=1, ncols=2)\n","    # Plot ROC\n","    for i in range(len(classes)):\n","        fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i], predicted_prob[:, i])\n","        ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\n","        ax[0].set(xlim=[-0.05, 1.0], ylim=[0.05, 1.05],\n","                xlabel=\"False Positive Rate\",\n","                ylabel=\"True Positive Rate (Recall)\",\n","                title=\"Receiver Operating Characteristic (ROC) Curve\")\n","        ax[0].legend(loc=\"lower right\")\n","        ax[0].grid(True)\n","\n","    # Plot Precision-Recall Curve\n","    for i in range(len(classes)):\n","        precision, recall, thresholds = metrics.precision_recall_curve(y_test_array[:,i], predicted_prob[:,i])\n","        ax[1].plot(recall, precision, lw=3, label='{0} (area={1:0.2f})'.format(classes[i], metrics.auc(recall, precision)))\n","        ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', ylabel=\"Precision\", title=\"Precision-Recall Curve\")\n","        ax[1].legend(loc=\"best\")\n","        ax[1].grid(True)\n","\n","    box = ax[1].get_position()\n","    box.x0 = box.x0 + 0.1\n","    box.x1 = box.x1 + 0.1\n","    ax[1].set_position(box)\n","    plt.show()\n","\n","#fit_Word2Vec_model(X, y, 'CLEAN_TEXT', 'CATEGORY','Random Forest')"]},{"cell_type":"markdown","metadata":{"id":"5fe6a120"},"source":["# Model Comparisons\n","\n","## Model with Word Embedding\n","\n","### Logistic Regression Model\n","\n","**Optimised Parameters from grid search:**  \n","LogisticRegression(C=0.05, class_weight='balanced', max_iter=10000)\n","\n","**Accuracy:** 0.79 | **AUC:** 0.93\n","\n","|           | Precision | Recall  | F1-Score | Support |\n","|-----------|-----------|---------|----------|---------|\n","| Class b   | 0.78      | 0.76    | 0.77     | 23069   |\n","| Class e   | 0.88      | 0.85    | 0.87     | 30700   |\n","| Class m   | 0.65      | 0.78    | 0.71     | 9037    |\n","| Class t   | 0.76      | 0.75    | 0.76     | 21660   |\n","| **Average** | **0.80**  | **0.79** | **0.79** | **84466** |\n","\n","**Macro Avg:** Precision: 0.77 | Recall: 0.79 | F1-Score: 0.78  \n","\n","**Weighted Avg:** Precision: 0.80 | Recall: 0.79 | F1-Score: 0.79 | Support: 84466\n","\n","---\n","\n","### One-vs-Rest Classifier\n","\n","**Optimised Parameters from grid search:**  \n","OneVsRestClassifier(estimator=SGDClassifier(alpha=1e-05, loss='log_loss', penalty='l1'))\n","\n","**Accuracy:** 0.80 | **AUC:** 0.93\n","\n","|           | Precision | Recall  | F1-Score | Support |\n","|-----------|-----------|---------|----------|---------|\n","| Class b   | 0.76      | 0.80    | 0.78     | 23095   |\n","| Class e   | 0.84      | 0.89    | 0.87     | 30579   |\n","| Class m   | 0.80      | 0.66    | 0.72     | 9077    |\n","| Class t   | 0.79      | 0.73    | 0.76     | 21715   |\n","| **Average** | **0.80**  | **0.80** | **0.80** | **84466** |\n","\n","**Macro Avg:** Precision: 0.79 | Recall: 0.77 | F1-Score: 0.78  \n","\n","**Weighted Avg:** Precision: 0.80 | Recall: 0.80 | F1-Score: 0.80 | Support: 84466\n","\n","---\n","\n","### Random Forest Classifier\n","\n","**Optimised Parameters from grid search:**  \n","RandomForestClassifier(criterion='entropy', max_depth=5, n_estimators=10)\n","\n","**Accuracy:** 0.63 | **AUC:** 0.85\n","\n","|           | Precision | Recall  | F1-Score | Support |\n","|-----------|-----------|---------|----------|---------|\n","| Class b   | 0.64      | 0.61    | 0.63     | 23172   |\n","| Class e   | 0.59      | 0.92    | 0.71     | 30538   |\n","| Class m   | 0.86      | 0.16    | 0.27     | 9108    |\n","| Class t   | 0.75      | 0.44    | 0.56     | 21648   |\n","| **Average** | **0.71**  | **0.53** | **0.54** | **84466** |\n","\n","**Macro Avg:** Precision: 0.71 | Recall: 0.53 | F1-Score: 0.54  \n","\n","**Weighted Avg:** Precision: 0.67 | Recall: 0.63 | F1-Score: 0.60 | Support: 84466\n","\n","---\n","\n","The comparison of the three models using word embeddings (embedding Model with Logistic Regression, embedding Model with One-vs-Rest Classifier, and embedding Model with Random Forest Classifier) suggests that these models generally perform worse than the initial bag of words models. Despite leveraging the semantic meaning of words through word embeddings, these models struggle to achieve comparable performance on the classification task.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"69d3e9f5"},"outputs":[],"source":["def main():\n","        barplot_target_category(df,'CATEGORY', {'b': 'BUISNESS', 't': 'TECHNOLOGY' , 'm' :'MEDICAL', 'e':'ENTERTAINMENT'})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"567ba20b"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5026b70"},"outputs":[],"source":["!pip install bert-tensorflow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23e93cc2"},"outputs":[],"source":["import pandas as pd\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","import tensorflow_hub as hub\n","from datetime import datetime\n","from sklearn.model_selection import train_test_split\n","import os\n","\n","print(\"tensorflow version : \", tf.__version__)\n","print(\"tensorflow_hub version : \", hub.__version__)\n","\n","import bert\n","from bert import run_classifier\n","from bert import optimization\n","from bert import tokenization\n","from bert import modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7f48ba7b"},"outputs":[],"source":["!pip install tensorflow.contrib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94293767"},"outputs":[],"source":["!pip install tensorflow_hub"]},{"cell_type":"markdown","metadata":{"id":"44fbac83"},"source":["# LTSM"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1692285676221,"user":{"displayName":"James Finbarr Kelly","userId":"03678949091753552422"},"user_tz":-60},"id":"e65337fd"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n","from keras.models import Sequential\n","from sklearn.feature_extraction.text import CountVectorizer\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","from tensorflow import keras"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":38},"id":"s0Hb7LzKF1d4"},"outputs":[{"data":{"text/html":["\n","     <input type=\"file\" id=\"files-2018c2c4-de8d-4a7c-be2a-4945a83907bc\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2018c2c4-de8d-4a7c-be2a-4945a83907bc\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["from google.colab import files\n","data_to_load = files.upload()"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":355},"executionInfo":{"elapsed":309,"status":"error","timestamp":1692285682934,"user":{"displayName":"James Finbarr Kelly","userId":"03678949091753552422"},"user_tz":-60},"id":"6866ba77","outputId":"e5541edd-d326-429a-ea48-f3016c962f9c"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-f1c590889025>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdownload_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uci-news-aggregator.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_frame_pre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TITLE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-598abd321910>\u001b[0m in \u001b[0;36mdownload_Data\u001b[0;34m(file_location)\u001b[0m\n\u001b[1;32m     14\u001b[0m     '''\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'uci-news-aggregator.csv'"]}],"source":["df = download_Data('uci-news-aggregator.csv')\n","df = data_frame_pre_process(df, 'TITLE', True, False, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9d7c6db"},"outputs":[],"source":["data= df.sample(frac = 0.75)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e53894d8","outputId":"e8160509-1d73-4728-8cd1-be122abf2c68"},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 40326 unique tokens.\n"]}],"source":["# The maximum number of words to be used. (most frequent)\n","MAX_NB_WORDS = 50000\n","# Max number of words in each complaint.\n","MAX_SEQUENCE_LENGTH = 250\n","# This is fixed.\n","EMBEDDING_DIM = 100\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=True)\n","tokenizer.fit_on_texts(data['CLEAN_TEXT'].values)\n","word_index = tokenizer.word_index\n","print('Found %s unique tokens.' % len(word_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2f57e815","outputId":"31ab75f5-5359-491b-b807-863584844c39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of data tensor: (316813, 250)\n"]}],"source":["X = tokenizer.texts_to_sequences(data['CLEAN_TEXT'].values)\n","X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n","print('Shape of data tensor:', X.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"309b257c","outputId":"6c887885-49d7-4017-d527-74e42b91a936"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of label tensor: (316813, 4)\n"]}],"source":["Y = pd.get_dummies(data['CATEGORY']).values\n","print('Shape of label tensor:', Y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d1304401","outputId":"3a1567eb-ad56-43fe-99c7-334aee64e434"},"outputs":[{"name":"stdout","output_type":"stream","text":["(221769, 250) (221769, 4)\n","(95044, 250) (95044, 4)\n"]}],"source":["X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n","print(X_train.shape,Y_train.shape)\n","print(X_test.shape,Y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dc7b69fc"},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n","model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(4, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"28f36889","outputId":"e0771665-4abb-4a72-c8c1-99aa41517684"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 250, 100)          5000000   \n","                                                                 \n"," lstm (LSTM)                 (None, 100)               80400     \n","                                                                 \n"," dense (Dense)               (None, 4)                 404       \n","                                                                 \n","=================================================================\n","Total params: 5080804 (19.38 MB)\n","Trainable params: 5080804 (19.38 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b906becd"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0554837b","outputId":"18f9b6eb-80a9-4f06-a2b8-35af856e36e5"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tensorboard extension is already loaded. To reload it, use:\n","  %reload_ext tensorboard\n"]}],"source":["%load_ext tensorboard"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e22987ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","  93/2426 [>.............................] - ETA: 51:53 - loss: 1.1950 - accuracy: 0.4906"]}],"source":["epochs = 5\n","batch_size = 128\n","\n","history = model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6f312f5c","outputId":"441fc333-1a03-4fc0-86a3-dbf2e4576b04"},"outputs":[{"data":{"text/html":["\n","      <iframe id=\"tensorboard-frame-bf5535754e59700b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n","      </iframe>\n","      <script>\n","        (function() {\n","          const frame = document.getElementById(\"tensorboard-frame-bf5535754e59700b\");\n","          const url = new URL(\"/\", window.location);\n","          const port = 6006;\n","          if (port) {\n","            url.port = port;\n","          }\n","          frame.src = url;\n","        })();\n","      </script>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["%tensorboard --logdir logs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a44df274"},"outputs":[],"source":["accr = model.evaluate(X_test,Y_test)\n","print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: 0.9501'.format(accr[0],accr[1]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3df57531"},"outputs":[],"source":["plt.title('Loss')\n","plt.plot(history.history['loss'], label='train')\n","plt.plot(history.history['val_loss'], label='test')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02b66018"},"outputs":[],"source":["plt.title('Accuracy')\n","plt.plot(history.history['acc'], label='train')\n","plt.plot(history.history['val_acc'], label='test')\n","plt.legend()\n","plt.show();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9be7160e"},"outputs":[],"source":["def check(new_title):\n","    seq = tokenizer.texts_to_sequences(new_title)\n","    padded = pad_sequences(seq, maxlen=MAX_SEQUENCE_LENGTH)\n","    pred = model.predict(padded)\n","    labels=['b','e','m','t']\n","    print(pred)\n","    print('\\n')\n","    Dict = {'b': 'buisness', 'm': 'Medical', 'e': 'Entertainment','t': 'technology'}\n","    return (Dict[labels[np.argmax(pred)]])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6fa87cca"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}
